{"neutrino": {"2309.12282": "2023-09-21, **Life Cycle Analysis of the GRAND Experiment**, Leidy T. Vargas-Ib\u00e1\u00f1ez et.al., [2309.12282v1](http://arxiv.org/abs/2309.12282v1)\n\n The goal of our study is to assess the environmental impact of the installation and use of the Giant Radio Array for Neutrino Detection (GRAND) prototype detection units, based on the life cycle assessment (LCA) methodology, and to propose recommendations that contribute to reduce the environmental impacts of the project at later stages. The functional unit, namely the quantified description of the studied system and of the performance requirements it fulfills, is to detect radio signals autonomously during 20 years, with 300 detection units deployed over 200 km^2 in the Gansu province in China (corresponding to the prototype GRANDProto300). We consider four main phases: the extraction of the materials and the production of the detection units (upstream phases), the use and the end-of-life phases (downstream phases), with transportation between each step. An inventory analysis is performed for the seven components of each detection unit, based on transparent assumptions. Most of the inventory data are taken from the Idemat2021 database (Industrial Design & Engineering Materials). Our results show that the components with the highest environmental impact are the antenna structure and the battery. The most pregnant indicators are `resource use', mineral and metals'; `resource use, fossils'; `ionizing radiation, human health'; `climate change'; and `acidification'. Therefore, the actions that we recommend in the first place aim at reducing the impact of these components. They include limiting the mass of the raw material used in the antenna, changing the alloy of the antenna, considering another type of battery with an extended useful life, and the use of recycled materials for construction. As a pioneering study applying the LCA methodology to a large-scale physics experiment, this work can serve as a basis for future assessments by other collaborations.\n\n", "2309.12264": "2023-09-21, **GRB221009A gamma-ray events from non-standard neutrino self-interactions**, Mansi Dhuria et.al., [2309.12264v1](http://arxiv.org/abs/2309.12264v1)\n\n The flux of high-energy astrophysical neutrinos observed by the present generation of neutrino detectors has already indicated a few hints of new physics beyond the Standard Model. In this work, we show that high-energy gamma-ray observations can also be considered as a complementary probe for unveiling the source of high-energy astrophysical neutrino events and new physics. Recently, the LHAASO collaboration has reported O(5000) gamma-ray events in the energy range between 0.5 TeV -18 TeV from gamma-ray burst GRB221009A within 2000 seconds after the initial outburst. We showed that attenuated high-energy gamma rays can be produced from the interaction of astrophysical neutrinos with CMB neutrinos through non-standard self-interaction of neutrinos mediated by light scalar bosons. The non-standard interaction of neutrinos recently took a lot of attention in cosmology for its role in reducing Hubble tension. We have constrained the parameter space of non-standard self-interacting neutrinos from the flux of photons observed by LHAASO and showed consistency of the same with the resulting parameter space from Hubble tension requirements and other recent constraints from laboratory/cosmology.\n\n", "2309.12258": "2023-09-21, **Primordial Black Hole Neutrinogenesis of Sterile Neutrino Dark Matter**, Muping Chen et.al., [2309.12258v1](http://arxiv.org/abs/2309.12258v1)\n\n We present a new mechanism, the evaporation of primordial black holes (PBHs), for the cosmological production of sterile neutrinos mixing with Standard Model active neutrinos, contributing to the dark matter. PBH sterile neutrinogenesis can produce a significant relic abundance with arbitrarily small active-sterile mixing. We concentrate on PBHs that matter dominate the Universe and identify the parameter space where decays of these sterile neutrinos could explain the putative 3.5 keV signal or potential signals in future experiments e.g. XRISM or beyond NuSTAR. The association of X-ray and gravitational wave signals is a unique feature of this scenario.\n\n", "2309.12249": "2023-09-21, **Impact of scalar NSI on the neutrino mass hierarchy sensitivity at DUNE, T2HK and T2HKK**, Arnab Sarker et.al., [2309.12249v1](http://arxiv.org/abs/2309.12249v1)\n\n The study of neutrino non-standard interactions (NSI) is a well-motivated phenomenological scenario to explore new physics beyond the Standard Model. The possible scalar coupling of neutrinos ($\\nu$) with matter is one of such new physics scenarios which appears as a sub-dominant effect that can impact the $\\nu$-oscillations in matter. The presence of scalar NSI introduces an additional contribution directly to the $\\nu$-mass matrix in the interaction Hamiltonian and subsequently to the $\\nu$-oscillations. This indicates that scalar NSI may have a significant impact on measurements related to $\\nu$-oscillations e.g. leptonic CP phase $(\\delta\\_{CP})$, $\\theta\\_{23}$ octant and neutrino mass hierarchy (MH). The linear scaling of the effects of scalar NSI with matter density also motivates its exploration in long-baseline (LBL) experiments. In this paper, we study the impact of a scalar-mediated NSI on the MH sensitivity of DUNE, T2HK and T2HKK, which are upcoming LBL experiments. We observe that the MH sensitivities of these experiments are affected by scalar NSI. For certain chosen values of scalar NSI elements, we note an enhancement in the standard MH sensitivities. We then combine the data from DUNE with T2HK/T2HKK to explore possible synergy among these experiments in a wider parameter space. We also observe a significant enhancement in the MH sensitivities for the combined analysis.\n\n", "2309.12130": "2023-09-21, **Search for Continuous and Transient Neutrino Emission Associated with IceCube's Highest-Energy Tracks: An 11-Year Analysis**, R. Abbasi et.al., [2309.12130v1](http://arxiv.org/abs/2309.12130v1)\n\n IceCube alert events are neutrinos with a moderate-to-high probability of having astrophysical origin. In this study, we analyze 11 years of IceCube data and investigate 122 alert events and a selection of high-energy tracks detected between 2009 and the end of 2021, searching for additional continuous and transient neutrino emission within their error regions. We find no evidence for significant continuous neutrino emission from any of the alert event directions. The only locally significant neutrino emission is the transient emission associated with the blazar TXS~0506+056, with a local significance of $ 3 \\sigma$, which confirms previous IceCube studies. When correcting for 122 test positions, the global p-value is $0.156$ and is compatible with the background hypothesis. We constrain the total continuous flux emitted from all 122 test positions at 100~TeV to be below $1.2 \\times 10^{-15}$~(TeV cm$^2$ s)$^{-1}$ at 90\\% confidence assuming an $E^{-2}$ spectrum, which corresponds to 4.5\\% of IceCube's astrophysical diffuse flux. Overall, we find no indication that alert events, in general, are linked to lower-energetic continuous or transient neutrino emission.\n\n", "2309.12091": "2023-09-21, **Scotoelectroweak theory**, Phung Van Dong et.al., [2309.12091v1](http://arxiv.org/abs/2309.12091v1)\n\n We argue that the higher weak isospin $SU(3)\\_L$ manifestly unifies dark matter and normal matter in its isomultiplets for which dark matter carries a conserved dark charge while normal matter does not. The resultant gauge symmetry is given by $SU(3)\\_C\\otimes SU(3)\\_L \\otimes U(1)\\_X\\otimes U(1)\\_G$, where the first factor is the color group, while the rest defines a theory of scotoelectroweak in which $X$ and $G$ determine electric charge $Q=T\\_3-1/\\sqrt{3}T\\_8+X$ and dark charge $D=-2/\\sqrt{3}T\\_3+G$. This setup provides both appropriate scotogenic neutrino masses and dark matter stability as preserved by a residual dark parity $P\\_D=(-1)^D$. Interpretation of the dark charge is further discussed, given that $SU(3)\\_L$ is broken at very high energy scale.\n\n", "2309.12086": "2023-09-21, **Outflow energy and black-hole spin evolution in collapsar scenarios**, Masaru Shibata et.al., [2309.12086v1](http://arxiv.org/abs/2309.12086v1)\n\n We explore the collapsar scenario for long gamma-ray bursts by performing axisymmetric neutrino-radiation magnetohydrodynamics simulations in full general relativity for the first time. In this paper, we pay particular attention to the outflow energy and the evolution of the black-hole spin. We show that for a strong magnetic field with an aligned field configuration initially given, a jet is launched by magnetohydrodynamical effects before the formation of a disk and a torus, and after the jet launch, the matter accretion onto the black hole is halted by the strong magnetic pressure, leading to the spin-down of the black hole due to the Blandford-Znajek mechanism. The spin-down timescale depends strongly on the magnetic-field strength initially given because the magnetic-field strength on the black-hole horizon, which is determined by the mass infall rate at the jet launch, depends strongly on the initial condition, although the total jet-outflow energy appears to be huge $>10^{53}$ erg depending only weakly on the initial field strength and configuration. For the models in which the magnetic-field configuration is not suitable for quick jet launch, a torus is formed and after a long-term magnetic-field amplification, a jet can be launched. For this case, the matter accretion onto the black hole continues even after the jet launch and black-hole spin-down is not found. We also find that the jet launch is often accompanied with the powerful explosion of the entire star with the explosion energy of order $10^{52}$ erg by magnetohydrodynamical effects. We discuss an issue of the overproduced energy for the early-jet-launch models.\n\n", "2309.11844": "2023-09-21, **Constructing the Hyper-Kamiokande Computing Model in the Build Up to Data Taking**, Sophie King et.al., [2309.11844v1](http://arxiv.org/abs/2309.11844v1)\n\n Hyper-Kamiokande is a next-generation multi-purpose neutrino experiment with a primary focus on constraining CP-violation in the lepton sector. It features a diverse science programme that includes neutrino oscillation studies, astrophysics, neutrino cross-section measurements, and searches for physics beyond the standard model, such as proton decay. Building on its predecessor, Super-Kamiokande, the Hyper-Kamiokande far detector has a total volume approximately 5 times larger and is estimated to collect nearly 2PB of data per year. The experiment will also include both on- and off-axis near detectors, including an Intermediate Water Cherenkov Detector. To manage the significant demands relating to the data from these detectors, and the associated Monte Carlo simulations for a range of physics studies, an efficient and scalable distributed computing model is essential. This model leverages Worldwide LHC Grid computing infrastructure and utilises the GridPP DIRAC instance for both workload management and for file cataloguing. In this report we forecast the computing requirements for the Hyper-K experiment, estimated to reach around 35PB (per replica) and 8,700 CPU cores (~ 100,000 HS06) by 2036. We outline the resources, tools, and workflow in place to satisfy this demand.\n\n"}, "dark matter": {"2309.12292": "2023-09-21, **Constraining dark energy cosmologies with spatial curvature using Supernovae JWST forecasting**, Pablo M. Maldonado Alonso et.al., [2309.12292v1](http://arxiv.org/abs/2309.12292v1)\n\n Recent cosmological tensions, in particular, to infer the local value of the Hubble constant $H\\_0$, have developed new independent techniques to constrain cosmological parameters in several cosmologies. Moreover, even when the concordance Cosmological Constant Cold Dark Matter ($\\Lambda$CDM) model has been well constrained with local observables, its physics has shown deviations from a flat background. Therefore, to explore a possible deviation from a flat $\\Lambda$CDM model that could explain the $H\\_0$ value in tension with other techniques, in this paper we study new cosmological constraints in spatial curvature dark energy models. Additionally, to standard current Supernovae Type Ia (SNIa) catalogs, we extend the empirical distance ladder method through an SNIa sample using the capabilities of the James Webb Space Telescope (JWST) to forecast SNIa up to $z \\sim 6$, with information on the star formation rates at high redshift. Furthermore, we found that our constraints provide an improvement in the statistics associated with $\\Omega\\_{m}$ when combining SNIa Pantheon and SNIa Pantheon+ catalogs with JW forecasting data.\n\n", "2309.12166": "2023-09-21, **Dark Sector Effective Field Theory**, Jin-Han Liang et.al., [2309.12166v1](http://arxiv.org/abs/2309.12166v1)\n\n We introduce the effective field theory of two different light dark particles interacting with the standard model (SM) light states in a single vertex, termed dark sector effective field theory (DSEFT). We focus on the new light particles with spin up to 1 and being real in essence, namely, new real scalars $\\phi$ and $S$, Majorana fermions $\\chi$ and $\\psi$, and real vectors $X\\_\\mu$ and $V\\_\\mu$. In the framework of low energy effective field theory with QED and QCD symmetry, the DSEFT can be classified into six categories, including the scalar-scalar-SM ($\\phi S$-SM), fermion-fermion-SM ($\\chi\\psi$-SM), vector-vector-SM ($X V$-SM), scalar-fermion-SM ($\\phi \\chi$-SM), scalar-vector-SM ($\\phi X$-SM), and fermion-vector-SM ($\\chi X$-SM) cases. For each case, we construct the effective operator basis up to canonical dimension 7, which will cover most interesting phenomenology at low energy. As a phenomenological example, we investigate the longstanding neutron lifetime anomaly through the neutron dark decay modes $n \\to \\chi \\phi \\text{ or } \\chi X$ from the effective interactions in the fermion-scalar-SM or fermion-vector-SM case. When treating the light fermion as a dark matter candidate, we also explore the constraints from DM-neutron annihilation signal at Super-Kamiokande. We find the neutron dark decay in each scenario can accommodate the anomaly, at the same time, without contradicting with the Super-Kamiokande limit.\n\n", "2309.12098": "2023-09-21, **Tunable Rectangular Resonant Cavities for Axion Haloscopes**, Ben T. McAllister et.al., [2309.12098v1](http://arxiv.org/abs/2309.12098v1)\n\n Axions are a compelling dark matter candidate, and one of the primary techniques employed to search for them is the axion haloscope, in which a resonant cavity is deployed inside a strong magnetic field so that some of the surrounding axions may convert into photons via the inverse Primakoff effect and become trapped inside the resonator. Resonant cavity design is critical to the sensitivity of a haloscope, and several geometries have been utilised and proposed. Here we consider a relatively simple concept - a rectangular resonant cavity with a tunable wall - and compare it to the standard tuning rod-type resonators employed in the field. We find that the rectangular cavities support similar modes to cylindrical tuning rod cavities, and have some advantages in terms of axion sensitivity and practicality, particularly when moving to higher frequencies which are of great and growing interest in the international axion dark matter community.\n\n", "2309.12043": "2023-09-21, **Dark Matter Annihilation via Breit-Wigner Enhancement with Heavier Mediator**, Yu Cheng et.al., [2309.12043v1](http://arxiv.org/abs/2309.12043v1)\n\n We propose a new scenario that both the dark matter freeze-out in the early Universe and its possible annihilation for indirect detection around a supermassive black hole are enhanced by a Breit-Wigner resonance. With the mediator mass larger than the total initial dark matter mass, this annihilation is almost forbidden at late times. Thus, the stringent cosmic microwave background and indirect detection constraints do not apply. However, a supermassive black hole can accelerate the dark matter particles to reactivate this resonant annihilation whose subsequent decay to photons leaves a unique signal. The running Fermi-LAT and the future COSI satellites can test this scenario.\n\n", "2309.11958": "2023-09-21, **Seeing dark matter via acceleration radiation**, Syed Masood A. S. Bukhari et.al., [2309.11958v1](http://arxiv.org/abs/2309.11958v1)\n\n Notwithstanding an impressive $\\sim 27\\%$ share of the total energy budget of our Universe, dark matter (DM) has by far remained elusive for any direct observations. Given its ubiquity, there is a genuine expectation that astrophysical black holes (BHs) surrounded by DM should leave imprints on the gravitational waves germinating from BH mergers. Theoretical models of DM offer a landscape of possibilities. Of these, perfect fluid dark matter (PDFM) is a novel candidate model which has been of considerable interest recently. In this Letter, employing the well-known \\textit{quantum optical} approach, we investigate the possibility of catching DM signatures via acceleration radiation emitted by a detector (e.g. an atom) falling freely within a PFDM-surrounded Schwarzschild BH.   The setup comprises a Casimir-type apparatus where the detector interacts with the field and excites thereby in a typical Unruh manner. We observe that our DM candidate, though offering contributions to the spacetime geometry on a classical footing, could however leave its potential quantum imprints in the radiation flux. Specifically, one notes that compared to a pure Schwarzschild BH, PFDM can markedly reduce particle emission as long as its density remains below a critical value, and vice versa. This novel study, bringing DM into Casimir physics, may possibly provide insights into the future table-top experiments in analogue gravity paradigm.\n\n", "2309.11905": "2023-09-21, **Electrons densities and plasma mass densities observed by pulsar dispersion in different locations of our galaxy**, Y. Ben-Aryeh et.al., [2309.11905v1](http://arxiv.org/abs/2309.11905v1)\n\n Electrons densities in different locations of our galaxy are obtained in pulsar astronomy by dividing the dispersion Measure by the distance of the pulsar to Earth. In order that such measurements will be reliable these distances should be obtained by methods which are independent of the electron density model and the results of such measurements are used in the present article to derive plasma mass densities. Our main analysis is based on the idea that the pulsars measurements are obtained for completely ionized hydrogen plasma. We use the electrons densities to derive the plasma mass densities and compare it with dark matter mass densities which are found to be much larger. But some factors which may reduce this difference are discussed. The properties of the low-density plasma are analyzed by using Saha equation.\n\n", "2309.11743": "2023-09-21, **The Hydrostatic Mass of A478: Discrepant Results From Chandra, NuSTAR, and XMM-Newton**, Cicely Potter et.al., [2309.11743v1](http://arxiv.org/abs/2309.11743v1)\n\n Galaxy clusters are the most recently formed and most massive, gravitationally bound structures in the universe. The number of galaxy clusters formed is highly dependent on cosmological parameters, such as the dark matter density, $\\sigma\\_8$, and $\\Omega\\_m$. The number density is a function of the cluster mass, which can be estimated from the density and temperature profiles of the intracluster medium (ICM) under the assumption of hydrostatic equilibrium. The temperature of the plasma, hence its mass, is calculated from the X-ray spectra. However, effective area calibration uncertainties in the soft band result in significantly different temperature measurements from various space-based X-ray telescopes. NuSTAR is potentially less susceptible to these issues than Chandra and XMM-Newton, having larger effective area, particularly at higher energies, enabling high precision temperature measurements. In this work, we present analyses of Chandra, NuSTAR, and XMM-Newton data of Abell 478 to investigate the nature of this calibration discrepancy. We find that NuSTAR temperatures are on average $\\sim$11% lower than that of Chandra, and XMM-Newton temperatures are on average $\\sim$5% lower than that of NuSTAR. This results in a NuSTAR mass at $r\\_{2500,Chandra}$ of $M\\_{2500,NuSTAR}=3.39^{+0.07}\\_{-0.07}\\times10^{14}$ $M\\_{\\odot}$, which is $\\sim$10% lower than that of $M\\_{2500,Chandra}$ and $\\sim$4% higher than $M\\_{2500,XMM-Newton}$.\n\n"}, "supernova": {"2309.12292": "2023-09-21, **Constraining dark energy cosmologies with spatial curvature using Supernovae JWST forecasting**, Pablo M. Maldonado Alonso et.al., [2309.12292v1](http://arxiv.org/abs/2309.12292v1)\n\n Recent cosmological tensions, in particular, to infer the local value of the Hubble constant $H\\_0$, have developed new independent techniques to constrain cosmological parameters in several cosmologies. Moreover, even when the concordance Cosmological Constant Cold Dark Matter ($\\Lambda$CDM) model has been well constrained with local observables, its physics has shown deviations from a flat background. Therefore, to explore a possible deviation from a flat $\\Lambda$CDM model that could explain the $H\\_0$ value in tension with other techniques, in this paper we study new cosmological constraints in spatial curvature dark energy models. Additionally, to standard current Supernovae Type Ia (SNIa) catalogs, we extend the empirical distance ladder method through an SNIa sample using the capabilities of the James Webb Space Telescope (JWST) to forecast SNIa up to $z \\sim 6$, with information on the star formation rates at high redshift. Furthermore, we found that our constraints provide an improvement in the statistics associated with $\\Omega\\_{m}$ when combining SNIa Pantheon and SNIa Pantheon+ catalogs with JW forecasting data.\n\n", "2309.12049": "2023-09-21, **Constraint on the event rate of general relativistic instability supernovae from the early JWST deep field data**, Takashi J. Moriya et.al., [2309.12049v1](http://arxiv.org/abs/2309.12049v1)\n\n General relativistic instability supernovae at ~10 < z < ~15 are predicted to be observed as red faint point sources, and they can be detected only in the reddest filters in JWST/NIRCam (F444W and F356W). They should be observed as persistent point sources with little flux variations for a couple of decades because of time dilation. We search for static point sources detected only in the F444W filter or only in the F444W and F356W filters in the early JWST deep field data. No real point source of such kind is identified. Therefore, the general relativistic instability supernova rate at ~10 < z < ~15 is constrained to be less than ~ 8e-7 Mpc-3 yr-1 for the first time.\n\n", "2309.12048": "2023-09-21, **Cosmology with multiple galaxies**, Chaitanya Chawak et.al., [2309.12048v1](http://arxiv.org/abs/2309.12048v1)\n\n Recent works have discovered a relatively tight correlation between $\\Omega\\_{\\rm m}$ and properties of individual simulated galaxies. Because of this, it has been shown that constraints on $\\Omega\\_{\\rm m}$ can be placed using the properties of individual galaxies while accounting for uncertainties on astrophysical processes such as feedback from supernova and active galactic nuclei. In this work, we quantify whether using the properties of multiple galaxies simultaneously can tighten those constraints. For this, we train neural networks to perform likelihood-free inference on the value of two cosmological parameters ($\\Omega\\_{\\rm m}$ and $\\sigma\\_8$) and four astrophysical parameters using the properties of several galaxies from thousands of hydrodynamic simulations of the CAMELS project. We find that using properties of more than one galaxy increases the precision of the $\\Omega\\_{\\rm m}$ inference. Furthermore, using multiple galaxies enables the inference of other parameters that were poorly constrained with one single galaxy. We show that the same subset of galaxy properties are responsible for the constraints on $\\Omega\\_{\\rm m}$ from one and multiple galaxies. Finally, we quantify the robustness of the model and find that without identifying the model range of validity, the model does not perform well when tested on galaxies from other galaxy formation models.\n\n", "2309.12026": "2023-09-21, **The Imaging X-ray Polarimetry Explorer (IXPE): Prospects for Spatially-Resolved X-ray Polarimetry of Extended Sources and In-Orbit Calibrations**, Riccardo Ferrazzoli et.al., [2309.12026v1](http://arxiv.org/abs/2309.12026v1)\n\n This thesis focuses on X-ray polarimetry and its recent resurgence due to the NASA/ASI Imaging X-ray Polarimetry Explorer (IXPE) mission launched in December 2021. It aims to address two critical tasks: in-orbit calibrations and observing faint extended celestial sources. For in-orbit calibrations, IXPE carries polarized and unpolarized calibration sources because known celestial sources, like the Crab Nebula, aren't suitable due to variability. Tests were performed on Flight Models of these calibration sources, ensuring their functionality in thermal vacuum when combined with IXPE detectors. Concerning observing faint extended sources, instrumental and diffuse backgrounds pose challenges. The impact of various background sources, such as instrumental, diffuse Galactic plane emission, and cosmic X-ray background, on X-ray polarimetry of faint, extended sources was examined. The feasibility study also covered observing two extended sources: the Tycho supernova remnant and the molecular clouds of the Sgr A complex. Monte Carlo simulations were employed to evaluate the detectability of polarization. Results show that the on-board calibration system will assess and verify the GPD's functionality in orbit. Background mitigation techniques will be required for faintest extended sources. For sources like Cas A, Tycho, and PSW MSH 15-52, background effects are negligible. In conclusion, the IXPE mission opens up spatially resolved X-ray polarimetry, enabling the determination of polarization angles and degrees. Preliminary data after launch indicates that on-board calibration sources are performing as expected, and imaging capabilities meet requirements. This research is essential for advancing X-ray polarimetry and understanding high-energy celestial sources.\n\n", "2309.12018": "2023-09-21, **Viscous fluid dynamics with decaying vacuum energy density**, C. P. Singh et.al., [2309.12018v1](http://arxiv.org/abs/2309.12018v1)\n\n In this work, we investigate the dynamics of bulk viscous models with decaying vacuum energy density (VED) in a spatially homogeneous and isotropic flat Friedmann-Lema\\^{i}tre- Robertson-walker (FLRW) spacetime. We particularly are interested to study the viscous model which considers first order deviation from equilibrium, i.e., the Eckart theory. In the first part, using the different forms of the bulk viscous coefficient, we find the main cosmological parameters, like Hubble parameter, scale factor, deceleration parameter and equation of state parameter analytically. We discuss some cosmological consequences of the evolutions and dynamics of the different viscous models with decaying VED. We examine the linear perturbation growth in the context of the bulk viscous model with decaying VED to see if it survives this further level of scrutiny. The second part of the work is devoted to constrain the viscous model of the form $\\zeta \\propto H$, where $\\zeta$ is the bulk viscous coefficient and $H$ is the Hubble parameter, using three different combinations of data from type Ia supernovae (Pantheon), $H(z)$ (cosmic chronometers), Baryon Acoustic Oscillation and $f(z)\\sigma\\_8(z)$ measurements with Markov Chain Monte Carlo (MCMC) method. We show that the considered model is compatible with the cosmological probes, and the $\\Lambda$CDM recovered in late-time of the evolution of the Universe. Finally, we obtain selection information criteria (AIC and BIC) to study the stability of the models.\n\n", "2309.11810": "2023-09-21, **Extragalactic Test of General Relativity from Strong Gravitational Lensing by using Artificial Neural Networks**, Jing-Yu Ran et.al., [2309.11810v1](http://arxiv.org/abs/2309.11810v1)\n\n This study aims to test the validity of general relativity (GR) on kiloparsec scales by employing a newly compiled galaxy-scale strong gravitational lensing (SGL) sample. We utilize the distance sum rule within the Friedmann-Lema\\^{\\i}tre-Robertson-Walker metric to obtain cosmology-independent constraints on both the parameterized post-Newtonian parameter $\\gamma\\_{\\rm PPN}$ and the spatial curvature $\\Omega\\_{k}$, which overcomes the circularity problem induced by the presumption of a cosmological model grounded in GR. To calibrate the distances in the SGL systems, we introduce a novel nonparametric approach, Artificial Neural Network (ANN), to reconstruct a smooth distance--redshift relation from the Pantheon+ sample of type Ia supernovae. Our results show that $\\gamma\\_{\\rm PPN}=1.16\\_{-0.12}^{+0.15}$ and $\\Omega\\_k=0.89\\_{-1.00}^{+1.97}$, indicating a spatially flat universe with the conservation of GR (i.e., $\\Omega\\_k=0$ and $\\gamma\\_{\\rm PPN}=1$) is basically supported within $1\\sigma$ confidence level. Assuming a zero spatial curvature, we find $\\gamma\\_{\\rm PPN}=1.09\\_{-0.10}^{+0.11}$, representing an agreement with the prediction of 1 from GR to a 9.6\\% precision. If we instead assume GR holds (i.e., $\\gamma\\_{\\rm PPN}=1$), the curvature parameter constraint can be further improved to be $\\Omega\\_k=0.11\\_{-0.47}^{+0.78}$. These resulting constraints demonstrate the effectiveness of our method in testing GR on galactic scales by combining observations of strong lensing and the distance--redshift relation reconstructed by ANN.\n\n"}}